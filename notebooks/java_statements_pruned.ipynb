{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from gensim.models import Word2Vec, FastText\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "__type = 'full'\n",
    "# __type = 'pruned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read(path: str):\n",
    "    codes = []\n",
    "    tags = []\n",
    "    paths = []\n",
    "    for i in tqdm.tqdm(glob.iglob(os.path.join(path, '*.json')), desc='Reading data', total=len(os.listdir(path)) // 3):\n",
    "        pre, ext = os.path.splitext(i)\n",
    "        tags.append(json.load(open(i, 'r', encoding='utf-8'))['Tags'])\n",
    "        statements = open(pre + '.java.ast.stm.flat', 'r', encoding='utf-8').read().strip('\\n').split('\\n\\n')\n",
    "        codes.append([])\n",
    "        paths.append(i)\n",
    "        for stm in statements:\n",
    "            if len(stm) > 200:\n",
    "                continue\n",
    "            codes[-1].append([])\n",
    "            for line in stm.split('\\n'):\n",
    "                try:\n",
    "                    token, children = line.split('\\t')\n",
    "                    children = tuple(map(int, children.split())) if children else ()\n",
    "                    codes[-1][-1].append((token, children))\n",
    "                except:\n",
    "                    pass\n",
    "    return codes, tags, paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Reading data:   0%|          | 0/18179 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63888871394b4f2c9e0f90ff19101bc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codes, tags, paths = read(f'../data/java/{__type}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "    for statement in code:\n",
    "        for node, children in statement:\n",
    "            for child in children:\n",
    "                _ = statement[child]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "17667"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for code in codes:\n",
    "    for statement in code:\n",
    "        for token, _ in statement:\n",
    "            vocab.add(token)\n",
    "vocab = list(vocab)\n",
    "token_to_id = {j:i for i, j in enumerate(vocab)}\n",
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['number theory',\n 'math',\n 'games',\n 'shortest paths',\n 'dp',\n 'sortings',\n 'meet-in-the-middle',\n 'bitmasks',\n 'flows',\n 'trees',\n 'interactive',\n 'two pointers',\n 'geometry',\n 'strings',\n 'implementation',\n 'graphs',\n 'dsu',\n 'string suffix structures',\n 'matrices',\n 'combinatorics',\n 'data structures',\n 'dfs and similar',\n 'constructive algorithms',\n 'brute force',\n 'binary search',\n 'hashing',\n 'divide and conquer',\n 'greedy']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set()\n",
    "for tag_list in tags:\n",
    "    for tag in tag_list:\n",
    "        labels.add(tag)\n",
    "labels = list(labels)\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vector_size = 192"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(f'../embeddings/java/{__type}/stm.w2v')\n",
    "w2v_wv = w2v_model.wv\n",
    "w2v_embeddings = np.array([w2v_wv[i] if i in w2v_wv else np.zeros((vector_size,)) for i in vocab])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "ft_model = FastText.load(f'../embeddings/java/{__type}/stm.ft')\n",
    "ft_wv = ft_model.wv\n",
    "ft_embeddings = np.array([ft_wv[i] if i in ft_wv else np.zeros((vector_size,)) for i in vocab])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05564952  0.02046087  0.10647431 -0.01923187  0.00446714 -0.00236576\n",
      "  0.00533537 -0.0250175   0.10094999 -0.00020496]\n",
      "[-0.12320415  0.19479103 -0.41616592  0.6490167   0.5443552  -0.37005484\n",
      "  0.6032727   0.01902923 -0.35569012 -0.06282882]\n",
      "17667 17667\n",
      "17667 17667\n",
      "21777\n",
      "21777\n"
     ]
    }
   ],
   "source": [
    "print(w2v_embeddings[0][:10])\n",
    "print(ft_embeddings[0][:10])\n",
    "print(sum(i in w2v_wv for i in vocab), len(vocab))\n",
    "print(sum(i in ft_wv for i in vocab), len(vocab))\n",
    "print(len(w2v_model.wv.key_to_index))\n",
    "print(len(ft_model.wv.key_to_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "vectorized = []\n",
    "indices = []\n",
    "he_tags = []\n",
    "\n",
    "statements_hist = []\n",
    "nodes_hist = []\n",
    "max_code_len = -1\n",
    "max_statement_len = -1\n",
    "\n",
    "for code in codes:\n",
    "    vectorized.append([])\n",
    "    indices.append([])\n",
    "    max_code_len = max(max_code_len, len(code))\n",
    "    statements_hist.append(len(code))\n",
    "    for statement in code:\n",
    "        vectorized[-1].append([])\n",
    "        indices[-1].append([])\n",
    "        max_statement_len = max(max_statement_len, len(statement))\n",
    "        nodes_hist.append(len(statement))\n",
    "        for token, _ in statement:\n",
    "            vectorized[-1][-1].append(token_to_id[token] + 1)\n",
    "            indices[-1][-1].append(_)\n",
    "\n",
    "for tag_list in tags:\n",
    "    he_tags.append([0] * len(labels))\n",
    "    for tag in tag_list:\n",
    "        he_tags[-1][label_to_id[tag]] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6145, 15482, 10013, 14548], [16478, 15482, 10013, 1565, 10677, 10013, 4971, 8064, 10013, 15893, 17071], [10454, 5684, 10013, 5941, 7577, 10013, 5099], [14973, 14515, 4705, 10013, 788], [10454, 5684, 10013, 5941, 7577, 10013, 10639, 10781, 5514, 10044], [246, 1370, 1348, 6023, 819, 9324, 10013, 5099, 10781, 5514, 10044, 9797], [15806, 1370, 1348, 6023, 780, 9324, 10013, 5099, 9797, 9797], [14973, 14515, 6871, 10013, 5121, 10219, 10781, 5514, 10044], [13256, 7244, 6023, 12844, 9324, 10013, 5099], [14973, 14515, 6871, 10013, 5121, 10219, 10781, 5514, 10044], [13256, 9324, 10013, 10639, 3034, 16412, 6023, 6880], [14973, 14515, 6871, 10013, 14828, 10219, 10781, 5514, 10044], [14973, 14515, 9120, 10013, 14054, 10219, 9324, 10013, 10639]]\n",
      "[[(1,), (2,), (3,), ()], [(1, 10), (2, 4), (3,), (), (5, 7), (6,), (), (8,), (9,), (), ()], [(1, 4), (2,), (3,), (), (5,), (6,), ()], [(1, 3), (2,), (), (4,), ()], [(1, 4), (2,), (3,), (), (5, 7), (6,), (), (8,), (9,), ()], [(1, 11), (2,), (3, 5, 8), (4,), (), (6,), (7,), (), (9,), (10,), (), ()], [(1, 8, 9), (2,), (3, 5), (4,), (), (6,), (7,), (), (), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1,), (2, 4), (3,), (), (5,), (6,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1,), (2, 4), (3,), (), (5,), (6,), (7,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()]]\n",
      "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized[0])\n",
    "print(indices[0])\n",
    "print(he_tags[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.77606028934485\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(statements_hist, bins=50)\n",
    "print(np.mean(statements_hist))\n",
    "print(max_code_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.177879379769255\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD7CAYAAACfQGjDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYbElEQVR4nO3df7DddX3n8edrg7hqKwmSpTSJTaypO4FZV7wL6dp2rLghgGPYHXWgzpJqxowrdHWnHQ11pjgqM9B2S8tW2aEmS3AcAku1ZErYmEU6zs5skPBDICDNFVGSARJJhHYdf0Tf+8f5pB4v55ub3JOce5M8HzNn7vf7/ny+57zP4XJf+f4456SqkCRpkH823Q1IkmYuQ0KS1MmQkCR1MiQkSZ0MCUlSJ0NCktRp0pBIsi7J7iSPTqj/XpJvJNme5I/76lcmGU/yRJLz++rLW208yZq++qIk97b6rUlObvWXt/XxNr7wiDxjSdIhO5Q9iZuA5f2FJL8NrADeWFVnAn/a6kuAS4Az2zafTTIrySzgM8AFwBLg0jYX4Frguqp6PbAPWNXqq4B9rX5dmydJGqGTJptQVV8d8K/4/wRcU1U/bHN2t/oKYEOrfyvJOHBOGxuvqicBkmwAViR5HHgb8DttznrgE8AN7b4+0eq3A3+ZJDXJu/9OO+20WrhwYruSpIO5//77v1tVcyfWJw2JDr8G/GaSq4EfAH9QVfcB84CtffN2thrA0xPq5wKvAb5XVfsHzJ93YJuq2p/khTb/uwdrbOHChWzbtm2KT0uSTkxJvj2oPtWQOAk4FVgK/BvgtiSvm+J9DS3JamA1wGtf+9rpakOSjjtTvbppJ/DF6vka8FPgNGAXsKBv3vxW66o/D8xOctKEOv3btPFT2vyXqKobq2qsqsbmzn3J3pIkaYqmGhJ/A/w2QJJfA06mdxhoI3BJuzJpEbAY+BpwH7C4Xcl0Mr2T2xvb+YV7gHe1+10J3NGWN7Z12vhXJjsfIUk6siY93JTkFuCtwGlJdgJXAeuAde2y2B8BK9sf8O1JbgMeA/YDl1fVT9r9XAFsBmYB66pqe3uIjwEbknwaeBBY2+prgc+3k9976QWLJGmEcrz943xsbKw8cS1JhyfJ/VU1NrHuO64lSZ0MCUlSJ0NCktTJkJAkdZrqm+kkFq65c8rbPnXNRUewE0lHi3sSkqROhoQkqZMhIUnqZEhIkjoZEpKkToaEJKmTISFJ6mRISJI6GRKSpE6GhCSpkyEhSepkSEiSOhkSkqROk4ZEknVJdrfvs5449vtJKslpbT1Jrk8ynuThJGf3zV2ZZEe7reyrvznJI22b65Ok1U9NsqXN35JkzpF5ypKkQ3UoexI3AcsnFpMsAJYB3+krXwAsbrfVwA1t7qnAVcC5wDnAVX1/9G8APtC33YHHWgPcXVWLgbvbuiRphCYNiar6KrB3wNB1wEeB6qutAG6unq3A7CRnAOcDW6pqb1XtA7YAy9vYq6tqa1UVcDNwcd99rW/L6/vqkqQRmdI5iSQrgF1V9fUJQ/OAp/vWd7baweo7B9QBTq+qZ9rys8DpU+lVkjR1h/3NdEleCfwhvUNNI1FVlaS6xpOspnd4i9e+9rWjakuSjntT2ZP4VWAR8PUkTwHzgQeS/BKwC1jQN3d+qx2sPn9AHeC5djiK9nN3V0NVdWNVjVXV2Ny5c6fwlCRJgxx2SFTVI1X1L6pqYVUtpHeI6OyqehbYCFzWrnJaCrzQDhltBpYlmdNOWC8DNrexF5MsbVc1XQbc0R5qI3DgKqiVfXVJ0ogcyiWwtwD/F3hDkp1JVh1k+ibgSWAc+CvgQwBVtRf4FHBfu32y1WhzPte2+SZwV6tfA/y7JDuAt7d1SdIITXpOoqounWR8Yd9yAZd3zFsHrBtQ3wacNaD+PHDeZP1Jko4e33EtSepkSEiSOhkSkqROhoQkqZMhIUnqZEhIkjoZEpKkTof92U2aeRauuXPK2z51zUVHsBNJxxv3JCRJnQwJSVInQ0KS1MmQkCR1MiQkSZ0MCUlSJ0NCktTJkJAkdTIkJEmdDAlJUqdD+Y7rdUl2J3m0r/YnSb6R5OEkX0oyu2/syiTjSZ5Icn5ffXmrjSdZ01dflOTeVr81ycmt/vK2Pt7GFx6pJy1JOjSHsidxE7B8Qm0LcFZV/Svg74ErAZIsAS4BzmzbfDbJrCSzgM8AFwBLgEvbXIBrgeuq6vXAPmBVq68C9rX6dW2eJGmEJg2JqvoqsHdC7ctVtb+tbgXmt+UVwIaq+mFVfQsYB85pt/GqerKqfgRsAFYkCfA24Pa2/Xrg4r77Wt+WbwfOa/MlSSNyJM5JvB+4qy3PA57uG9vZal311wDf6wucA/Wfu682/kKbL0kakaFCIsnHgf3AF45MO1PuY3WSbUm27dmzZzpbkaTjypRDIsnvAu8A3ltV1cq7gAV90+a3Wlf9eWB2kpMm1H/uvtr4KW3+S1TVjVU1VlVjc+fOnepTkiRNMKWQSLIc+Cjwzqr6ft/QRuCSdmXSImAx8DXgPmBxu5LpZHontze2cLkHeFfbfiVwR999rWzL7wK+0hdGkqQRmPSb6ZLcArwVOC3JTuAqelczvRzY0s4lb62qD1bV9iS3AY/ROwx1eVX9pN3PFcBmYBawrqq2t4f4GLAhyaeBB4G1rb4W+HyScXonzi85As9XknQYJg2Jqrp0QHntgNqB+VcDVw+obwI2Dag/Se/qp4n1HwDvnqw/SdLR4zuuJUmdDAlJUidDQpLUyZCQJHUyJCRJnQwJSVInQ0KS1MmQkCR1MiQkSZ0MCUlSJ0NCktTJkJAkdTIkJEmdDAlJUqdJPypc0s8sXHPnlLd96pqLjmAn0mi4JyFJ6mRISJI6GRKSpE6ThkSSdUl2J3m0r3Zqki1JdrSfc1o9Sa5PMp7k4SRn922zss3fkWRlX/3NSR5p21yf9qXZXY8hSRqdQ9mTuAlYPqG2Bri7qhYDd7d1gAuAxe22GrgBen/wgauAc+l9n/VVfX/0bwA+0Lfd8kkeQ5I0IpOGRFV9Fdg7obwCWN+W1wMX99Vvrp6twOwkZwDnA1uqam9V7QO2AMvb2KuramtVFXDzhPsa9BiSpBGZ6iWwp1fVM235WeD0tjwPeLpv3s5WO1h954D6wR5DOiYNc/kseAmtpsfQJ67bHkAdgV6m/BhJVifZlmTbnj17jmYrknRCmWpIPNcOFdF+7m71XcCCvnnzW+1g9fkD6gd7jJeoqhuraqyqxubOnTvFpyRJmmiqIbEROHCF0krgjr76Ze0qp6XAC+2Q0WZgWZI57YT1MmBzG3sxydJ2VdNlE+5r0GNIkkZk0nMSSW4B3gqclmQnvauUrgFuS7IK+DbwnjZ9E3AhMA58H3gfQFXtTfIp4L4275NVdeBk+IfoXUH1CuCuduMgjyFJGpFJQ6KqLu0YOm/A3AIu77ifdcC6AfVtwFkD6s8PegxJ0uj4jmtJUidDQpLUyZCQJHUyJCRJnQwJSVInQ0KS1MmQkCR1MiQkSZ0MCUlSJ0NCktTJkJAkdTIkJEmdDAlJUidDQpLUyZCQJHUyJCRJnQwJSVKnSb+ZTjreLFxz53S3IB0zhtqTSPJfkmxP8miSW5L88ySLktybZDzJrUlObnNf3tbH2/jCvvu5stWfSHJ+X315q40nWTNMr5KkwzflkEgyD/jPwFhVnQXMAi4BrgWuq6rXA/uAVW2TVcC+Vr+uzSPJkrbdmcBy4LNJZiWZBXwGuABYAlza5kqSRmTYw00nAa9I8mPglcAzwNuA32nj64FPADcAK9oywO3AXyZJq2+oqh8C30oyDpzT5o1X1ZMASTa0uY8N2fOM5CEQSTPRlPckqmoX8KfAd+iFwwvA/cD3qmp/m7YTmNeW5wFPt233t/mv6a9P2KarLkkakWEON82h9y/7RcAvA6+id7ho5JKsTrItybY9e/ZMRwuSdFwa5sT124FvVdWeqvox8EXgLcDsJAcOY80HdrXlXcACgDZ+CvB8f33CNl31l6iqG6tqrKrG5s6dO8RTkiT1GyYkvgMsTfLKdm7hPHrnC+4B3tXmrATuaMsb2zpt/CtVVa1+Sbv6aRGwGPgacB+wuF0tdTK9k9sbh+hXknSYpnziuqruTXI78ACwH3gQuBG4E9iQ5NOttrZtshb4fDsxvZfeH32qanuS2+gFzH7g8qr6CUCSK4DN9K6cWldV26faryTp8A11dVNVXQVcNaH8JD+7Oql/7g+Ad3fcz9XA1QPqm4BNw/QoSZo6P5ZDktTJkJAkdfKzmyQdNcO8SfSpay46gp1oqtyTkCR1MiQkSZ083KRjkp91JY2GIXGE+EdL0vHIw02SpE6GhCSpkyEhSepkSEiSOhkSkqROhoQkqZMhIUnqZEhIkjoZEpKkToaEJKmTH8shHSP82G1Nh6H2JJLMTnJ7km8keTzJryc5NcmWJDvazzltbpJcn2Q8ycNJzu67n5Vt/o4kK/vqb07ySNvm+iQZpl9J0uEZ9nDTXwD/q6r+JfBG4HFgDXB3VS0G7m7rABcAi9ttNXADQJJT6X1P9rn0vhv7qgPB0uZ8oG+75UP2K0k6DFMOiSSnAL8FrAWoqh9V1feAFcD6Nm09cHFbXgHcXD1bgdlJzgDOB7ZU1d6q2gdsAZa3sVdX1daqKuDmvvuSJI3AMHsSi4A9wP9I8mCSzyV5FXB6VT3T5jwLnN6W5wFP922/s9UOVt85oC5JGpFhQuIk4Gzghqp6E/D/+NmhJQDaHkAN8RiHJMnqJNuSbNuzZ8/RfjhJOmEMExI7gZ1VdW9bv51eaDzXDhXRfu5u47uABX3bz2+1g9XnD6i/RFXdWFVjVTU2d+7cIZ6SJKnflEOiqp4Fnk7yhlY6D3gM2AgcuEJpJXBHW94IXNaucloKvNAOS20GliWZ005YLwM2t7EXkyxtVzVd1ndfkqQRGPZ9Er8HfCHJycCTwPvoBc9tSVYB3wbe0+ZuAi4ExoHvt7lU1d4knwLua/M+WVV72/KHgJuAVwB3tZskaUSGComqeggYGzB03oC5BVzecT/rgHUD6tuAs4bpUZI0dX4shySpkyEhSepkSEiSOhkSkqROhoQkqZMhIUnqZEhIkjoZEpKkToaEJKmTISFJ6mRISJI6GRKSpE6GhCSpkyEhSepkSEiSOhkSkqROhoQkqZMhIUnqNHRIJJmV5MEkf9vWFyW5N8l4klvb91+T5OVtfbyNL+y7jytb/Ykk5/fVl7faeJI1w/YqSTo8R2JP4sPA433r1wLXVdXrgX3AqlZfBexr9evaPJIsAS4BzgSWA59twTML+AxwAbAEuLTNlSSNyFAhkWQ+cBHwubYe4G3A7W3KeuDitryirdPGz2vzVwAbquqHVfUtYBw4p93Gq+rJqvoRsKHNlSSNyLB7En8OfBT4aVt/DfC9qtrf1ncC89ryPOBpgDb+Qpv/T/UJ23TVJUkjMuWQSPIOYHdV3X8E+5lqL6uTbEuybc+ePdPdjiQdN4bZk3gL8M4kT9E7FPQ24C+A2UlOanPmA7va8i5gAUAbPwV4vr8+YZuu+ktU1Y1VNVZVY3Pnzh3iKUmS+k05JKrqyqqaX1UL6Z14/kpVvRe4B3hXm7YSuKMtb2zrtPGvVFW1+iXt6qdFwGLga8B9wOJ2tdTJ7TE2TrVfSdLhO2nyKYftY8CGJJ8GHgTWtvpa4PNJxoG99P7oU1Xbk9wGPAbsBy6vqp8AJLkC2AzMAtZV1faj0K8kqcMRCYmq+jvg79ryk/SuTJo45wfAuzu2vxq4ekB9E7DpSPQoSTp8vuNaktTJkJAkdTIkJEmdDAlJUidDQpLU6WhcAitphlm45s4pb/vUNRcdwU50rHFPQpLUyT2JE9ww/8KUdPxzT0KS1MmQkCR1MiQkSZ0MCUlSJ0NCktTJkJAkdTIkJEmdDAlJUidDQpLUyZCQJHWackgkWZDkniSPJdme5MOtfmqSLUl2tJ9zWj1Jrk8ynuThJGf33dfKNn9HkpV99TcneaRtc32SDPNkJUmHZ5g9if3A71fVEmApcHmSJcAa4O6qWgzc3dYBLgAWt9tq4AbohQpwFXAuve/GvupAsLQ5H+jbbvkQ/UqSDtOUQ6KqnqmqB9ryPwCPA/OAFcD6Nm09cHFbXgHcXD1bgdlJzgDOB7ZU1d6q2gdsAZa3sVdX1daqKuDmvvuSJI3AETknkWQh8CbgXuD0qnqmDT0LnN6W5wFP9222s9UOVt85oC5JGpGhQyLJLwB/DXykql7sH2t7ADXsYxxCD6uTbEuybc+ePUf74STphDFUSCR5Gb2A+EJVfbGVn2uHimg/d7f6LmBB3+bzW+1g9fkD6i9RVTdW1VhVjc2dO3eYpyRJ6jPM1U0B1gKPV9Wf9Q1tBA5cobQSuKOvflm7ymkp8EI7LLUZWJZkTjthvQzY3MZeTLK0PdZlffclSRqBYb6Z7i3AfwQeSfJQq/0hcA1wW5JVwLeB97SxTcCFwDjwfeB9AFW1N8mngPvavE9W1d62/CHgJuAVwF3tJkkakSmHRFX9H6DrfQvnDZhfwOUd97UOWDegvg04a6o9SpKG4zuuJUmdhjncdNxZuObO6W5BkmYU9yQkSZ0MCUlSJ0NCktTJkJAkdTIkJEmdDAlJUidDQpLUyfdJSJqRhn3f0lPXXHSEOjmxuSchSepkSEiSOhkSkqROhoQkqZMhIUnq5NVNko5Lw1wd5ZVRP+OehCSpkyEhSeo040MiyfIkTyQZT7JmuvuRpBPJjA6JJLOAzwAXAEuAS5Msmd6uJOnEMaNDAjgHGK+qJ6vqR8AGYMU09yRJJ4yZfnXTPODpvvWdwLnT1IukE4RXRv3MTA+JQ5JkNbC6rf5jkieO4sOdBnz3KN7/0TDjes61k06ZcT1P4ljrFw6x50P4bzVKM/51HvB6zfiem18ZVJzpIbELWNC3Pr/Vfk5V3QjcOIqGkmyrqrFRPNaRYs9H37HWL9jzqByLPfeb6eck7gMWJ1mU5GTgEmDjNPckSSeMGb0nUVX7k1wBbAZmAeuqavs0tyVJJ4wZHRIAVbUJ2DTdffQZyWGtI8yej75jrV+w51E5Fnv+J6mq6e5BkjRDzfRzEpKkaWRIDJBkQZJ7kjyWZHuSDw+Y89YkLyR5qN3+aDp6ndDTU0keaf1sGzCeJNe3jzh5OMnZ09Fn6+UNfa/dQ0leTPKRCXOm/TVOsi7J7iSP9tVOTbIlyY72c07HtivbnB1JVk5zz3+S5Bvtv/uXkszu2Pagv0Mj7vkTSXb1/fe/sGPbafnono6eb+3r96kkD3VsOy2v85RUlbcJN+AM4Oy2/IvA3wNLJsx5K/C3093rhJ6eAk47yPiFwF1AgKXAvdPdc+trFvAs8Csz7TUGfgs4G3i0r/bHwJq2vAa4dsB2pwJPtp9z2vKcaex5GXBSW752UM+H8js04p4/AfzBIfzufBN4HXAy8PWJ/6+OsucJ4/8V+KOZ9DpP5eaexABV9UxVPdCW/wF4nN67v491K4Cbq2crMDvJGdPdFHAe8M2q+vZ0NzJRVX0V2DuhvAJY35bXAxcP2PR8YEtV7a2qfcAWYPnR6rPfoJ6r6stVtb+tbqX3nqMZo+N1PhTT9tE9B+s5SYD3ALeMopejyZCYRJKFwJuAewcM/3qSrye5K8mZo+1soAK+nOT+9i70iQZ9zMlMCL9L6P6faaa9xgCnV9UzbflZ4PQBc2bqaw3wfnp7lINM9js0ale0Q2TrOg7rzdTX+TeB56pqR8f4THudOxkSB5HkF4C/Bj5SVS9OGH6A3uGRNwL/DfibEbc3yG9U1dn0PjX38iS/Nd0NTaa9SfKdwP8cMDwTX+OfU71jB8fMJYJJPg7sB77QMWUm/Q7dAPwq8K+BZ+gdvjlWXMrB9yJm0ut8UIZEhyQvoxcQX6iqL04cr6oXq+of2/Im4GVJThtxmxN72tV+7ga+RG9XvN8hfczJiF0APFBVz00cmImvcfPcgcN07efuAXNm3Gud5HeBdwDvbeH2EofwOzQyVfVcVf2kqn4K/FVHLzPxdT4J+A/ArV1zZtLrPBlDYoB2PHEt8HhV/VnHnF9q80hyDr3X8vnRdfmSfl6V5BcPLNM7UfnohGkbgcvaVU5LgRf6DptMl85/cc2017jPRuDA1UorgTsGzNkMLEsypx0mWdZq0yLJcuCjwDur6vsdcw7ld2hkJpwv+/cdvczEj+55O/CNqto5aHCmvc6Tmu4z5zPxBvwGvUMIDwMPtduFwAeBD7Y5VwDb6V1NsRX4t9Pc8+taL19vfX281ft7Dr0vcfom8AgwNs09v4reH/1T+moz6jWmF2DPAD+md7x7FfAa4G5gB/C/gVPb3DHgc33bvh8Yb7f3TXPP4/SO3R/4ff7vbe4vA5sO9js0jT1/vv2ePkzvD/8ZE3tu6xfSuwLxm9Pdc6vfdOB3uG/ujHidp3LzHdeSpE4ebpIkdTIkJEmdDAlJUidDQpLUyZCQJHUyJCRJnQwJSVInQ0KS1On/A9YcJnr/zjd6AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(nodes_hist, bins=20)\n",
    "print(np.mean(nodes_hist))\n",
    "print(max_statement_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([1.405278e+06, 1.590000e+02, 6.000000e+00, 4.000000e+00,\n        0.000000e+00, 0.000000e+00, 7.000000e+00, 0.000000e+00,\n        1.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n        0.000000e+00, 0.000000e+00, 0.000000e+00, 2.000000e+00]),\n array([2.00000e+00, 1.02850e+02, 2.03700e+02, 3.04550e+02, 4.05400e+02,\n        5.06250e+02, 6.07100e+02, 7.07950e+02, 8.08800e+02, 9.09650e+02,\n        1.01050e+03, 1.11135e+03, 1.21220e+03, 1.31305e+03, 1.41390e+03,\n        1.51475e+03, 1.61560e+03, 1.71645e+03, 1.81730e+03, 1.91815e+03,\n        2.01900e+03]),\n <BarContainer object of 20 artists>)"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(nodes_hist, bins=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def get_embedding(shape: tuple, vocab: list, input_size: int, output_size: int, shift: int = 0, pre_trained = None):\n",
    "    embedding = tf.keras.layers.Embedding(input_size, output_size, name='Tree_Embedding', mask_zero=True)\n",
    "    embedding.build(shape)\n",
    "    embedding.trainable = pre_trained is None\n",
    "    if pre_trained is not None:\n",
    "        weights = np.zeros((input_size, output_size))\n",
    "        for i, token in enumerate(vocab):\n",
    "            try:\n",
    "                weights[i + shift] = pre_trained[i]\n",
    "            except:\n",
    "                pass\n",
    "        embedding.set_weights([weights])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_embedding_encoder(input_shape: tuple, output_size: int):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    outputs = tf.keras.layers.Dense(output_size)(inputs)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='Embedding_Encoder')\n",
    "\n",
    "\n",
    "def get_statement_encoder(input_shape: tuple, filters: int):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Lambda(lambda tensor: tf.math.reduce_max(tensor, axis=2))(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name='Statement_Encoder')\n",
    "\n",
    "\n",
    "def get_tree_encoder(input_shape: tuple, predict: bool = False):\n",
    "    inputs = tf.keras.layers.Input(input_shape, name='Inputs')\n",
    "    x = tf.keras.layers.Dropout(0.1, name='Embedding_Dropout')(inputs)\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.RNN(tf.keras.layers.GRUCell(units=128)),\n",
    "        name='Double_Bidirectional_GRU')(x)\n",
    "    if predict:\n",
    "        x = tf.keras.layers.Dense(units=len(labels), activation='sigmoid', name='Prediction')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x, name='Tree_Encoder')\n",
    "\n",
    "\n",
    "def get_combined_model(input_shape: tuple, embedding, encoder, statement_encoder, tree_encoder):\n",
    "    inputs = tf.keras.layers.Input(input_shape, name='Statement_Trees')\n",
    "    x = embedding(inputs)\n",
    "    x = encoder(x)\n",
    "    x = statement_encoder(x)\n",
    "    x = tree_encoder(x)\n",
    "    x = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.swish, name='Dense')(x)\n",
    "    x = tf.keras.layers.Dense(units=len(labels), activation='sigmoid', name='Prediction')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=x, name='PSLNN_Combined')\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_flat_model(input_shape: tuple, embedding):\n",
    "    inputs = tf.keras.layers.Input(input_shape, dtype=tf.int32, name='Nodes')\n",
    "    embedded = embedding(inputs)\n",
    "    dropout = tf.keras.layers.Dropout(0.5, name='Embedding_Dropout')(embedded)\n",
    "\n",
    "    n_layers = 2\n",
    "    kernels = [3, 5, 7]\n",
    "    layers = []\n",
    "\n",
    "    for k in kernels:\n",
    "        x = dropout\n",
    "        n = x.shape[-1]\n",
    "        for i in range(n_layers):\n",
    "            x = tf.keras.layers.Conv1D(n, k, activation=tf.keras.activations.swish, padding='same', name=f'Conv1D_{k}_{n}')(x)\n",
    "            x = tf.keras.layers.BatchNormalization(name=f'Batch_Norm_{k}_{n}')(x)\n",
    "            n *= 2\n",
    "\n",
    "        x = tf.keras.layers.GlobalMaxPooling1D(name=f'Max_Pool_{k}')(x)\n",
    "        layers.append(x)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='Pool_Concatenate')(layers)\n",
    "    x = tf.keras.layers.Dropout(0.5, name='Concatenate_Dropout')(x)\n",
    "    x = tf.keras.layers.Dense(units=512, activation=tf.keras.activations.swish, name=f'Dense')(x)\n",
    "    x = tf.keras.layers.Dense(units=len(labels), activation='sigmoid', name='Prediction')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=x, name='PSLNN_Flat')\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "class StatementDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, vectorized, indices, he_tags, batch_size):\n",
    "        self.vectorized = vectorized\n",
    "        self.indices = indices\n",
    "        self.he_tags = he_tags\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.vectorized) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "        vectorized_batch = vectorized[start:end]\n",
    "        indices_batch = indices[start:end]\n",
    "        he_tags_batch = he_tags[start:end]\n",
    "\n",
    "        max_len = max(len(j) for i in vectorized_batch for j in i)\n",
    "        code_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [tf.keras.preprocessing.sequence.pad_sequences(i, padding='post', maxlen=max_len) for i in vectorized_batch],\n",
    "            padding='post'\n",
    "        )\n",
    "        code_batch = code_batch[:,1:129]\n",
    "        indices_batch = [[i[j] for j in range(1, min(129, len(i)))] for i in indices_batch]\n",
    "        return (code_batch, indices_batch), he_tags_batch\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = np.arange(len(self.vectorized))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        self.vectorized = [self.vectorized[i] for i in indices]\n",
    "        self.indices = [self.indices[i] for i in indices]\n",
    "        self.he_tags = [self.he_tags[i] for i in indices]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.shuffle()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "shift = 1\n",
    "emb_input_size = len(vocab) + shift\n",
    "emb_output_size = 192\n",
    "emb_encoder_output_size = 128\n",
    "embedding = get_embedding(shape=(None, None, None),\n",
    "                          input_size=emb_input_size,\n",
    "                          output_size=emb_output_size,\n",
    "                          pre_trained=ft_embeddings,\n",
    "                          shift=1,\n",
    "                          vocab=vocab)\n",
    "embedding_encoder_model = get_embedding_encoder(input_shape=(None, None, emb_output_size),\n",
    "                                                output_size=emb_encoder_output_size)\n",
    "tree_encoder_model = get_tree_encoder(input_shape=(None, emb_encoder_output_size),\n",
    "                                      predict=True)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(-tf.math.reduce_sum(y_true * tf.math.log(y_pred), axis=-1))\n",
    "\n",
    "\n",
    "def tree_encoding_step(optimizer, code_batch, indices_batch, he_tags_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # b: code_batch length\n",
    "        # n: code_batch.shape[1] -- number of sequences in code\n",
    "        # m: code_batch.shape[2] -- number of statements in sequence\n",
    "        # e: embedding vector size\n",
    "        # code_batch: (b, n, m,)\n",
    "        # encoded: (b, n, m, e,)\n",
    "        encoded = embedding_encoder_model(embedding(code_batch), training=True)\n",
    "        encoded_as_list = [[[encoded[i, j, k] for k in range(code_batch.shape[2])] for j in range(code_batch.shape[1])] for i in range(len(code_batch))]\n",
    "        # statement encoding over known embeddings\n",
    "        for i in range(len(indices_batch)):\n",
    "            for j in range(len(indices_batch[i])):\n",
    "                for k in range(len(indices_batch[i][j]) - 1, -1, -1):\n",
    "                    children = indices_batch[i][j][k]\n",
    "                    if not children:\n",
    "                        continue\n",
    "                    encoded_as_list[i][j][k] = tf.reduce_sum([\n",
    "                        encoded_as_list[i][j][k],\n",
    "                        *(encoded_as_list[i][j][l] for l in children)\n",
    "                    ], axis=0)\n",
    "        pooled = tf.math.reduce_max(encoded_as_list, axis=2)\n",
    "        logits = tree_encoder_model(pooled, training=True)\n",
    "        loss_value = tf.math.reduce_mean(tf.keras.losses.binary_crossentropy(he_tags_batch, logits))\n",
    "        grads = tape.gradient(loss_value, tree_encoder_model.trainable_weights + embedding_encoder_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, tree_encoder_model.trainable_weights + embedding_encoder_model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "def models_parallel_train(epochs: int, batch_size: int):\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate=2e-3)\n",
    "    statement_dataset = StatementDataset(vectorized, indices, he_tags, batch_size=batch_size)\n",
    "    for epoch in tqdm.trange(epochs):\n",
    "        pbar = tqdm.tqdm(enumerate(statement_dataset), total=len(statement_dataset))\n",
    "        for step, ((code_batch, indices_batch), he_tags_batch) in pbar:\n",
    "            loss_value = tree_encoding_step(optimizer, code_batch, indices_batch, he_tags_batch).numpy()\n",
    "            pbar.set_description(str(loss_value))\n",
    "        statement_dataset.on_epoch_end()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae3e63207dc24fc3a93985788d2c19e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4545 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2aa6d5b49397466fadf267a084574a1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_parallel_train(epochs=1, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embedding_encoder_model.save('java_embedding_encoder_weights')\n",
    "tree_encoder_model.save('java_statement_tree_encoder_weights')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "def predict_on_batch(code_batch, indices_batch):\n",
    "    encoded = embedding_encoder_model(embedding(code_batch))\n",
    "    encoded_as_list = [[[encoded[i, j, k] for k in range(code_batch.shape[2])] for j in range(code_batch.shape[1])] for i in range(len(code_batch))]\n",
    "    for i in range(len(indices_batch)):\n",
    "        for j in range(len(indices_batch[i])):\n",
    "            for k in range(len(indices_batch[i][j]) - 1, -1, -1):\n",
    "                children = indices_batch[i][j][k]\n",
    "                if not children:\n",
    "                    continue\n",
    "                encoded_as_list[i][j][k] = tf.reduce_sum([\n",
    "                    encoded_as_list[i][j][k],\n",
    "                    *(encoded_as_list[i][j][l] for l in children)\n",
    "                ], axis=0)\n",
    "    pooled = tf.math.reduce_max(encoded_as_list, axis=2)\n",
    "    logits = tree_encoder_model(pooled)\n",
    "    return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def statement_encoding(epochs: int, batch_size: int):\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "#\n",
    "#     for epoch in tqdm.trange(epochs):\n",
    "#         vectorized_batches = batch(vectorized, batch_size)\n",
    "#         indices_batches = batch(indices, batch_size)\n",
    "#         he_tags_batches = batch(he_tags, batch_size)\n",
    "#         pbar = tqdm.tqdm(enumerate(zip(vectorized_batches, indices_batches, he_tags_batches)), total=len(vectorized) // batch_size)\n",
    "#         for step, (vectorized_batch, indices_batch, he_tags_batch) in pbar:\n",
    "#             max_len = max(len(j) for i in vectorized_batch for j in i)\n",
    "#             code_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#                 [tf.keras.preprocessing.sequence.pad_sequences(i, padding='post', maxlen=max_len) for i in vectorized_batch],\n",
    "#                 padding='post'\n",
    "#             )\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 # m: max_len\n",
    "#                 # code_batch: b x n x m\n",
    "#                 # encoded: b x n x m x e\n",
    "#                 encoded = encoder_model(embedding(code_batch), training=True)\n",
    "#                 pooled = statement_encoder_model(encoded, training=True)\n",
    "#                 #logits = model(pooled, training=True)\n",
    "#                 #loss_value = loss_fn(he_tags_batch, logits)\n",
    "#\n",
    "#             #grads = tape.gradient(loss_value, model.trainable_weights + encoder_model.trainable_weights)\n",
    "#             #optimizer.apply_gradients(zip(grads, model.trainable_weights + encoder_model.trainable_weights))\n",
    "#             #pbar.set_description(str(loss_value.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class FlatteningDataset(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, vectorized, indices, he_tags, batch_size):\n",
    "#         self.vectorized = vectorized\n",
    "#         self.indices = indices\n",
    "#         self.he_tags = he_tags\n",
    "#         self.batch_size = batch_size\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return (len(self.vectorized) + self.batch_size - 1) // self.batch_size\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         start = idx * self.batch_size\n",
    "#         end = (idx + 1) * self.batch_size\n",
    "#         vectorized_batch = vectorized[start:end]\n",
    "#         indices_batch = indices[start:end]\n",
    "#         he_tags_batch = he_tags[start:end]\n",
    "#\n",
    "#         vectorized_batch = [[st for sequence in code for st in sequence] for code in vectorized_batch]\n",
    "#         code_batch = tf.keras.preprocessing.sequence.pad_sequences(vectorized_batch, maxlen=1024, padding='post')\n",
    "#         return code_batch, np.array(he_tags_batch)\n",
    "#\n",
    "#     def on_epoch_end(self):\n",
    "#         pass\n",
    "\n",
    "# shift = 1\n",
    "# emb_input_size = len(vocab) + shift\n",
    "# emb_output_size = 192\n",
    "#\n",
    "# flat_model = get_flat_model(input_shape=(None,),\n",
    "#                             embedding=get_embedding(shape=(None, None),\n",
    "#                                                     vocab=vocab,\n",
    "#                                                     input_size=emb_input_size,\n",
    "#                                                     output_size=emb_output_size,\n",
    "#                                                     shift=shift,\n",
    "#                                                     pre_trained=ft_embeddings))\n",
    "#\n",
    "# flat_dataset = FlatteningDataset(vectorized, indices, he_tags, batch_size=8)\n",
    "#\n",
    "# emb_encoder_output_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}