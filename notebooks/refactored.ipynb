{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import tqdm.notebook as tqdm\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import tensorflow_addons as tfa\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read(path: str):\n",
    "    balanced_labels = [\n",
    "        'dp',\n",
    "        'greedy',\n",
    "        'implementation',\n",
    "        'dfs and similar',\n",
    "        'graphs',\n",
    "        'brute force',\n",
    "        'math',\n",
    "        'number theory',\n",
    "        'constructive algorithms',\n",
    "        'trees',\n",
    "        'binary search',\n",
    "        'data structures',\n",
    "        'two pointers',\n",
    "        '*special',\n",
    "        'sortings',\n",
    "        'strings',\n",
    "        'bitmasks',\n",
    "        'combinatorics',\n",
    "        'geometry',\n",
    "    ]\n",
    "    codes = []\n",
    "    tags = []\n",
    "    paths = []\n",
    "    for i in tqdm.tqdm(glob.iglob(os.path.join(path, '*.json')), desc='Reading data', total=(len(os.listdir(path)) - 1) // 3):\n",
    "        pre, ext = os.path.splitext(i)\n",
    "        tags.append(list(filter(lambda tag: tag in balanced_labels, json.load(open(i, 'r', encoding='utf-8'))['Tags'])))\n",
    "        statements = open(pre + '.java.ast.stm.flat', 'r', encoding='utf-8').read().strip('\\n').split('\\n\\n')\n",
    "        codes.append([])\n",
    "        paths.append(i)\n",
    "        for stm in statements:\n",
    "            if len(stm) > 200:\n",
    "                continue\n",
    "            codes[-1].append([])\n",
    "            for line in stm.split('\\n'):\n",
    "                try:\n",
    "                    token, children = line.split('\\t')\n",
    "                    children = tuple(map(int, children.split())) if children else ()\n",
    "                    codes[-1][-1].append((token, children))\n",
    "                except Exception as e:\n",
    "                    print(i)\n",
    "                    print(line)\n",
    "                    print()\n",
    "                    print(stm)\n",
    "                    raise e\n",
    "\n",
    "    # validate\n",
    "    for code, p in zip(codes, paths):\n",
    "        for statement in code:\n",
    "            for node, children in statement:\n",
    "                for child in children:\n",
    "                    try:\n",
    "                        _ = statement[child]\n",
    "                    except Exception as e:\n",
    "                        print(p)\n",
    "                        print(node)\n",
    "                        print(child)\n",
    "                        print(statement)\n",
    "                        raise e\n",
    "\n",
    "    return codes, tags, paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "codes, tags, paths = read(f'../data/revisited/java/full')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StatementTreeDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, vectorized, indices, he_tags, batch_size):\n",
    "        self.vectorized = vectorized\n",
    "        self.indices = indices\n",
    "        self.he_tags = he_tags\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.vectorized) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "        vectorized_batch = self.vectorized[start:end]\n",
    "        indices_batch = self.indices[start:end]\n",
    "        he_tags_batch = self.he_tags[start:end]\n",
    "\n",
    "        max_len = max(len(j) for i in vectorized_batch for j in i)\n",
    "        code_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [tf.keras.preprocessing.sequence.pad_sequences(i, padding='post', maxlen=max_len) for i in vectorized_batch],\n",
    "            padding='post'\n",
    "        )\n",
    "        code_batch = code_batch[:,1:129]\n",
    "        indices_batch = [[i[j] for j in range(1, min(129, len(i)))] for i in indices_batch]\n",
    "        return (code_batch, indices_batch), he_tags_batch\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = np.arange(len(self.vectorized))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        self.vectorized = [self.vectorized[i] for i in indices]\n",
    "        self.indices = [self.indices[i] for i in indices]\n",
    "        self.he_tags = [self.he_tags[i] for i in indices]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.shuffle()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class StatementTreeModel:\n",
    "    def __init__(self, codes, tags, name: str = 'STM'):\n",
    "        self.name = name\n",
    "        self.epochs = 0\n",
    "        self.embedding_layer = None\n",
    "        self.embedding_encoder = None\n",
    "        self.tree_encoder = None\n",
    "        self.optimizer = None\n",
    "\n",
    "        vocab = set()\n",
    "        for code in codes:\n",
    "            for statement in code:\n",
    "                for token, _ in statement:\n",
    "                    vocab.add(token)\n",
    "        self.__vocab = list(vocab)\n",
    "        self.__token_to_id = {j:i for i, j in enumerate(vocab)}\n",
    "\n",
    "        labels = set()\n",
    "        for tag_list in tags:\n",
    "            for tag in tag_list:\n",
    "                labels.add(tag)\n",
    "        self.__labels = list(labels)\n",
    "        self.__label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "        vectorized, indices, he_tags = [], [], []\n",
    "\n",
    "        # vectorize code, split tokens and children indices\n",
    "        for code in codes:\n",
    "            vectorized.append([])\n",
    "            indices.append([])\n",
    "            for statement in code:\n",
    "                vectorized[-1].append([])\n",
    "                indices[-1].append([])\n",
    "                for token, _ in statement:\n",
    "                    vectorized[-1][-1].append(self.__token_to_id[token] + 1)\n",
    "                    indices[-1][-1].append(_)\n",
    "\n",
    "        # labels hot-encoding\n",
    "        for tag_list in tags:\n",
    "            he_tags.append([0] * len(labels))\n",
    "            for tag in tag_list:\n",
    "                he_tags[-1][self.__label_to_id[tag]] = 1\n",
    "\n",
    "        # split data\n",
    "        self.__vectorized_train, self.__vectorized_test, \\\n",
    "        self.__indices_train, self.__indices_test, \\\n",
    "        self.__he_tags_train, self.__he_tags_test = \\\n",
    "            sklearn.model_selection.train_test_split(vectorized,\n",
    "                                                     indices,\n",
    "                                                     he_tags)\n",
    "\n",
    "        # load embeddings\n",
    "        self.__vocab_shift = 1\n",
    "        self.__embedding_input_size = len(vocab) + self.__vocab_shift\n",
    "        self.__embedding_output_size = 192\n",
    "        self.__embedding_encoder_output_size = 128\n",
    "        self.__embeddings = {'w2v': Word2Vec, 'ft': FastText}\n",
    "        for emb in self.__embeddings.keys():\n",
    "            model = self.__embeddings[emb].load(f'../embeddings/revisited/java/full/stm.{emb}')\n",
    "            wv = model.wv\n",
    "            self.__embeddings[emb] = np.array([wv[i] if i in wv else np.zeros((self.__embedding_output_size,)) for i in vocab])\n",
    "\n",
    "    def __create_embedding_layer(self, pre_trained = None):\n",
    "        embedding = tf.keras.layers.Embedding(self.__embedding_input_size, self.__embedding_output_size,\n",
    "                                              name='Tree_Embedding', mask_zero=True)\n",
    "        embedding.build((None, None, None))\n",
    "        embedding.trainable = pre_trained is None\n",
    "        if pre_trained is not None:\n",
    "            weights = np.zeros((self.__embedding_input_size, self.__embedding_output_size))\n",
    "            for i, token in enumerate(self.__vocab):\n",
    "                try:\n",
    "                    weights[i + self.__vocab_shift] = pre_trained[i]\n",
    "                except:\n",
    "                    pass\n",
    "            embedding.set_weights([weights])\n",
    "        return embedding\n",
    "\n",
    "    def __create_embedding_encoder(self):\n",
    "        inputs = tf.keras.layers.Input((None, None, self.__embedding_output_size))\n",
    "        outputs = tf.keras.layers.Dense(self.__embedding_encoder_output_size)(inputs)\n",
    "        return tf.keras.Model(inputs=inputs, outputs=outputs, name='Embedding_Encoder')\n",
    "\n",
    "    def __create_tree_encoder(self, backbone: str = 'cnn'):\n",
    "        inputs = tf.keras.layers.Input((None, self.__embedding_encoder_output_size), name='Inputs')\n",
    "        dropout = tf.keras.layers.Dropout(0.2, name='Embedding_Dropout')(inputs)\n",
    "        if backbone == 'rnn':\n",
    "            x = tf.keras.layers.Bidirectional(\n",
    "                    tf.keras.layers.RNN(tf.keras.layers.GRUCell(units=128)),\n",
    "                name='Double_Bidirectional_GRU')(dropout)\n",
    "        else:\n",
    "            n_layers = 4\n",
    "            kernels = [3, 5, 7]\n",
    "            layers = []\n",
    "            for k in kernels:\n",
    "                x = dropout\n",
    "                n = x.shape[-1]\n",
    "                for i in range(n_layers):\n",
    "                    x = tf.keras.layers.Conv1D(n, k, activation=tf.keras.activations.swish, padding='same', name=f'Conv1D_{k}_{n}')(x)\n",
    "                    x = tf.keras.layers.BatchNormalization(name=f'Batch_Norm_{k}_{n}')(x)\n",
    "                    n *= 2\n",
    "                x = tf.keras.layers.GlobalMaxPooling1D(name=f'Max_Pool_{k}')(x)\n",
    "                layers.append(x)\n",
    "            x = tf.keras.layers.Concatenate(axis=-1, name='Pool_Concatenate')(layers)\n",
    "            x = tf.keras.layers.Dropout(0.2, name='Concatenate_Dropout')(x)\n",
    "        x = tf.keras.layers.Dense(units=512, activation=tf.keras.activations.swish, name=f'Dense')(x)\n",
    "        x = tf.keras.layers.Dense(units=len(self.__labels), activation='sigmoid', name='Prediction')(x)\n",
    "        return tf.keras.models.Model(inputs=inputs, outputs=x, name='Tree_Encoder')\n",
    "\n",
    "    def __calc_logits(self, code_batch, indices_batch):\n",
    "        # b: code_batch length\n",
    "        # n: code_batch.shape[1] -- number of sequences in code\n",
    "        # m: code_batch.shape[2] -- number of statements in sequence\n",
    "        # e: embedding vector size\n",
    "        # code_batch: (b, n, m,)\n",
    "        # encoded: (b, n, m, e,)\n",
    "        encoded = self.embedding_encoder(self.embedding_layer(code_batch), training=True)\n",
    "        encoded_as_list = [[[encoded[i, j, k]\n",
    "                             for k in range(code_batch.shape[2])]\n",
    "                            for j in range(code_batch.shape[1])]\n",
    "                           for i in range(len(code_batch))]\n",
    "        # statement encoding over known embeddings\n",
    "        for i in range(len(indices_batch)):\n",
    "            for j in range(len(indices_batch[i])):\n",
    "                for k in range(len(indices_batch[i][j]) - 1, -1, -1):\n",
    "                    children = indices_batch[i][j][k]\n",
    "                    if not children:\n",
    "                        continue\n",
    "                    encoded_as_list[i][j][k] = tf.reduce_sum([\n",
    "                        encoded_as_list[i][j][k],\n",
    "                        *(encoded_as_list[i][j][l] for l in children)\n",
    "                    ], axis=0)\n",
    "        pooled = tf.math.reduce_max(encoded_as_list, axis=2)\n",
    "        logits = self.tree_encoder(pooled, training=True)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def __calc_loss(self, y_true, y_pred, metrics=None):\n",
    "        metrics = metrics or []\n",
    "        loss = tf.math.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "        for metric in metrics:\n",
    "            metric.update_state(y_true, y_pred)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def __tree_encoding_step(self, code_batch, indices_batch, he_tags_batch, metrics):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.__calc_logits(code_batch, indices_batch)\n",
    "            loss = self.__calc_loss(he_tags_batch, logits, metrics)\n",
    "            trainable_weights = self.tree_encoder.trainable_weights + self.embedding_encoder.trainable_weights\n",
    "        grads = tape.gradient(loss, trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "        return loss\n",
    "\n",
    "    def compile(self, backbone='rnn', lr=1e-3, embeddings: str = None):\n",
    "        pre_trained = self.__embeddings.get(embeddings, None)\n",
    "        self.optimizer = tf.keras.optimizers.Adamax(learning_rate=lr)\n",
    "        self.embedding_layer = self.__create_embedding_layer(pre_trained=pre_trained)\n",
    "        self.embedding_encoder = self.embedding_encoder or self.__create_embedding_encoder()\n",
    "        self.tree_encoder = self.tree_encoder or self.__create_tree_encoder(backbone=backbone)\n",
    "\n",
    "    def fit(self, epochs=5, batch_size=4):\n",
    "        # create datasets\n",
    "        train_dataset = StatementTreeDataset(self.__vectorized_train, self.__indices_train, self.__he_tags_train,\n",
    "                                             batch_size=batch_size)\n",
    "        test_dataset = StatementTreeDataset(self.__vectorized_test, self.__indices_test, self.__he_tags_test,\n",
    "                                            batch_size=batch_size)\n",
    "\n",
    "        # create logging handlers\n",
    "        os.makedirs(self.name, exist_ok=True)\n",
    "        train_loss_log = open(os.path.join(self.name, 'train_loss.txt'), 'w', encoding='utf-8')\n",
    "        train_f1_log   = open(os.path.join(self.name, 'train_f1.txt'),   'w', encoding='utf-8')\n",
    "        test_loss_log  = open(os.path.join(self.name, 'test_loss.txt'),  'w', encoding='utf-8')\n",
    "        test_f1_log    = open(os.path.join(self.name, 'test_f1.txt'),    'w', encoding='utf-8')\n",
    "        metrics = [tfa.metrics.F1Score(len(self.__labels), average='macro')]\n",
    "\n",
    "        try:\n",
    "            for epoch in range(epochs):\n",
    "\n",
    "                #train\n",
    "                loss = 0\n",
    "                pbar = tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset))\n",
    "                for step, ((code_batch, indices_batch), he_tags_batch) in pbar:\n",
    "                    loss_value = self.__tree_encoding_step(code_batch, indices_batch, he_tags_batch,\n",
    "                                                           metrics)\n",
    "                    loss += loss_value.numpy()\n",
    "                    mean_loss = f'{loss / (step + 1):.3f}'\n",
    "                    f1_score = f'{metrics[0].result().numpy():.3f}'\n",
    "                    pbar.set_description(f'[TRAIN {self.epochs}/{self.epochs}] | Loss: {mean_loss}; F1: {f1_score}')\n",
    "                    if step % 100 == 0:\n",
    "                        train_loss_log.write(f'{mean_loss} ')\n",
    "                        train_f1_log.write(f'{f1_score} ')\n",
    "                train_loss_log.write('\\n')\n",
    "                train_f1_log.write('\\n')\n",
    "\n",
    "                # save\n",
    "                self.epochs += 1\n",
    "                self.save()\n",
    "\n",
    "                # reset metrics\n",
    "                for metric in metrics:\n",
    "                    metric.reset_state()\n",
    "\n",
    "                # test\n",
    "                loss = 0\n",
    "                pbar = tqdm.tqdm(enumerate(test_dataset), total=len(test_dataset))\n",
    "                for step, ((code_batch, indices_batch), he_tags_batch) in pbar:\n",
    "                    logits = self.__calc_logits(code_batch, indices_batch)\n",
    "                    loss_value = self.__calc_loss(he_tags_batch, logits, metrics)\n",
    "                    loss += loss_value.numpy()\n",
    "                    mean_loss = f'{loss / (step + 1):.3f}'\n",
    "                    f1_score = f'{metrics[0].result().numpy():.3f}'\n",
    "                    pbar.set_description(f'[TEST] | Loss: {mean_loss}; F1: {f1_score}')\n",
    "\n",
    "                test_loss_log.write(f'{loss / len(test_dataset)}\\n')\n",
    "                test_f1_log.write(f'{metrics[0].result().numpy():.3f}\\n')\n",
    "\n",
    "                # reset metrics\n",
    "                for metric in metrics:\n",
    "                    metric.reset_state()\n",
    "\n",
    "                train_loss_log.flush()\n",
    "                train_f1_log.flush()\n",
    "                test_loss_log.flush()\n",
    "                test_f1_log.flush()\n",
    "                train_dataset.on_epoch_end()\n",
    "                test_dataset.on_epoch_end()\n",
    "        finally:\n",
    "            train_loss_log.close()\n",
    "            train_f1_log.close()\n",
    "            test_loss_log.close()\n",
    "            test_f1_log.close()\n",
    "\n",
    "    def save(self):\n",
    "        self.embedding_encoder.save(os.path.join(self.name, f'embedding_encoder_weights_{self.epochs}'))\n",
    "        self.tree_encoder.save(os.path.join(self.name, f'statement_tree_encoder_weights_{self.epochs}'))\n",
    "\n",
    "    def load(self, epochs, embedding_encoder_weights_path, tree_encoder_weights_path):\n",
    "        self.epochs = epochs\n",
    "        self.embedding_encoder = tf.keras.models.load_model(f'{embedding_encoder_weights_path}')\n",
    "        self.tree_encoder = tf.keras.models.load_model(f'{tree_encoder_weights_path}')\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.embedding_encoder.summary())\n",
    "        print(self.tree_encoder.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = StatementTreeModel(codes=codes, tags=tags, name='STM_drop02_conv4n_drop02_dense512')\n",
    "model.compile(lr=5e-4, embeddings='ft', backbone='cnn')\n",
    "model.summary()\n",
    "model.fit(epochs=3, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}