{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import tensorflow_addons as tfa\n",
    "from gensim.models import Word2Vec, FastText\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "__type = 'full'\n",
    "# __type = 'pruned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read(path: str):\n",
    "    balanced_labels = [\n",
    "        'dp',\n",
    "        'greedy',\n",
    "        'implementation',\n",
    "        'dfs and similar',\n",
    "        'graphs',\n",
    "        'brute force',\n",
    "        'math',\n",
    "        'number theory',\n",
    "        'constructive algorithms',\n",
    "        'trees',\n",
    "        'binary search',\n",
    "        'data structures',\n",
    "        'two pointers',\n",
    "        '*special',\n",
    "        'sortings',\n",
    "        'strings',\n",
    "        'bitmasks',\n",
    "        'combinatorics',\n",
    "        'geometry',\n",
    "    ]\n",
    "    codes = []\n",
    "    tags = []\n",
    "    paths = []\n",
    "    for i in tqdm.tqdm(glob.iglob(os.path.join(path, '*.json')), desc='Reading data', total=(len(os.listdir(path)) - 1) // 3):\n",
    "        pre, ext = os.path.splitext(i)\n",
    "        tags.append(list(filter(lambda tag: tag in balanced_labels, json.load(open(i, 'r', encoding='utf-8'))['Tags'])))\n",
    "        statements = open(pre + '.java.ast.stm.flat', 'r', encoding='utf-8').read().strip('\\n').split('\\n\\n')\n",
    "        codes.append([])\n",
    "        paths.append(i)\n",
    "        for stm in statements:\n",
    "            if len(stm) > 200:\n",
    "                continue\n",
    "            codes[-1].append([])\n",
    "            for line in stm.split('\\n'):\n",
    "                try:\n",
    "                    token, children = line.split('\\t')\n",
    "                    children = tuple(map(int, children.split())) if children else ()\n",
    "                    codes[-1][-1].append((token, children))\n",
    "                except Exception as e:\n",
    "                    print(i)\n",
    "                    print(line)\n",
    "                    print()\n",
    "                    print(stm)\n",
    "                    raise e\n",
    "\n",
    "    return codes, tags, paths"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Reading data:   0%|          | 0/48537 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "961fc63eba394e8c8001dc20e02c7d7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "codes, tags, paths = read(f'../data/revisited/java/{__type}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "for code, p in zip(codes, paths):\n",
    "    for statement in code:\n",
    "        for node, children in statement:\n",
    "            for child in children:\n",
    "                try:\n",
    "                    _ = statement[child]\n",
    "                except Exception as e:\n",
    "                    print(p)\n",
    "                    print(node)\n",
    "                    print(child)\n",
    "                    print(statement)\n",
    "                    raise e"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "46343"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for code in codes:\n",
    "    for statement in code:\n",
    "        for token, _ in statement:\n",
    "            vocab.add(token)\n",
    "vocab = list(vocab)\n",
    "token_to_id = {j:i for i, j in enumerate(vocab)}\n",
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(19,\n defaultdict(int,\n             {'dp': 8642,\n              'greedy': 13835,\n              'implementation': 20837,\n              'dfs and similar': 3067,\n              'graphs': 2986,\n              'brute force': 7896,\n              'math': 14851,\n              'number theory': 5538,\n              'constructive algorithms': 11079,\n              'trees': 1945,\n              'binary search': 2635,\n              'data structures': 4172,\n              'two pointers': 1612,\n              '*special': 1852,\n              'sortings': 4213,\n              'strings': 4391,\n              'bitmasks': 1299,\n              'combinatorics': 1274,\n              'geometry': 1237}))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "freq = defaultdict(int)\n",
    "labels = set()\n",
    "for tag_list in tags:\n",
    "    for tag in tag_list:\n",
    "        labels.add(tag)\n",
    "        freq[tag] += 1\n",
    "labels = list(labels)\n",
    "label_to_id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "len(labels), freq"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "vector_size = 192"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load(f'../embeddings/revisited/java/{__type}/stm.w2v')\n",
    "w2v_wv = w2v_model.wv\n",
    "w2v_embeddings = np.array([w2v_wv[i] if i in w2v_wv else np.zeros((vector_size,)) for i in vocab])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ft_model = FastText.load(f'../embeddings/revisited/java/{__type}/stm.ft')\n",
    "ft_wv = ft_model.wv\n",
    "ft_embeddings = np.array([ft_wv[i] if i in ft_wv else np.zeros((vector_size,)) for i in vocab])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00996109  0.01853905  0.04357075 -0.06149104  0.04350245  0.0757494\n",
      "  0.10162805 -0.05165766  0.02721444 -0.12090312]\n",
      "[-16.046976   -1.534591   20.914219    8.221931    1.2179637 -15.312166\n",
      " -40.591694   23.785963  -38.204758  -37.188934 ]\n",
      "46343 46343\n",
      "46343 46343\n",
      "54385\n",
      "54385\n"
     ]
    }
   ],
   "source": [
    "print(w2v_embeddings[0][:10])\n",
    "print(ft_embeddings[0][:10])\n",
    "print(sum(i in w2v_wv for i in vocab), len(vocab))\n",
    "print(sum(i in ft_wv for i in vocab), len(vocab))\n",
    "print(len(w2v_model.wv.key_to_index))\n",
    "print(len(ft_model.wv.key_to_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "vectorized = []\n",
    "indices = []\n",
    "he_tags = []\n",
    "\n",
    "statements_hist = []\n",
    "nodes_hist = []\n",
    "max_code_len = -1\n",
    "max_statement_len = -1\n",
    "\n",
    "for code in codes:\n",
    "    vectorized.append([])\n",
    "    indices.append([])\n",
    "    max_code_len = max(max_code_len, len(code))\n",
    "    statements_hist.append(len(code))\n",
    "    for statement in code:\n",
    "        vectorized[-1].append([])\n",
    "        indices[-1].append([])\n",
    "        max_statement_len = max(max_statement_len, len(statement))\n",
    "        nodes_hist.append(len(statement))\n",
    "        for token, _ in statement:\n",
    "            vectorized[-1][-1].append(token_to_id[token] + 1)\n",
    "            indices[-1][-1].append(_)\n",
    "\n",
    "for tag_list in tags:\n",
    "    he_tags.append([0] * len(labels))\n",
    "    for tag in tag_list:\n",
    "        he_tags[-1][label_to_id[tag]] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42904, 37495, 39348, 31392], [17598, 37495, 39348, 39579, 31813, 39348, 15407, 369, 39348, 27426, 38499], [20827, 27039, 39348, 31504, 26696, 39348, 28003], [20153, 13402, 7760, 39348, 13578], [20827, 27039, 39348, 31504, 26696, 39348, 43119, 32881, 19995, 23058], [16983, 6594, 27508, 12262, 17431, 45960, 39348, 28003, 32881, 19995, 23058, 6957], [24890, 6594, 27508, 12262, 34159, 45960, 39348, 28003, 6957, 6957], [20153, 13402, 20363, 39348, 34040, 6506, 32881, 19995, 23058], [40782, 13707, 12262, 17876, 45960, 39348, 28003], [20153, 13402, 20363, 39348, 34040, 6506, 32881, 19995, 23058], [40782, 45960, 39348, 43119, 41822, 30129, 12262, 33392], [20153, 13402, 20363, 39348, 31533, 6506, 32881, 19995, 23058], [20153, 13402, 43739, 39348, 11792, 6506, 45960, 39348, 43119]]\n",
      "[[(1,), (2,), (3,), ()], [(1, 10), (2, 4), (3,), (), (5, 7), (6,), (), (8,), (9,), (), ()], [(1, 4), (2,), (3,), (), (5,), (6,), ()], [(1, 3), (2,), (), (4,), ()], [(1, 4), (2,), (3,), (), (5, 7), (6,), (), (8,), (9,), ()], [(1, 11), (2,), (3, 5, 8), (4,), (), (6,), (7,), (), (9,), (10,), (), ()], [(1, 8, 9), (2,), (3, 5), (4,), (), (6,), (7,), (), (), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1,), (2, 4), (3,), (), (5,), (6,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1,), (2, 4), (3,), (), (5,), (6,), (7,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()], [(1, 3, 5), (2,), (), (4,), (), (6,), (7,), (8,), ()]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized[0])\n",
    "print(indices[0])\n",
    "print(he_tags[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.84589076374724\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT4ElEQVR4nO3df4xdZ33n8fdn7ZJ2adM4ZGoZO10baliFqHXACkEtVbYpiRMqHFaI2rtqXBphEIkWtpW6zvaPsHQjhV0g20isW1O8cVYQkxJorGDqGi8qqtQEj0vk2PlRTxKnGcuJp0lKdkuVxfS7f9xn2IMzY4/nXs/YM++XdDXnfM9zzn0enyifec45906qCknS/PbPZrsDkqTZZxhIkgwDSZJhIEnCMJAkAQtnuwPTddFFF9Xy5ctnuxuSdE7Zt2/f31XV0In1czYMli9fzvDw8Gx3Q5LOKUmemajuZSJJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJHEOfwK5H8s3fW3C+uHb3z3DPZGks4MzA0mSYSBJMgwkSUwhDJJsTXIsyYFO7UtJHm6vw0kebvXlSf6xs+0PO/u8LckjSUaS3JkkrX5hkt1JDrWfi87AOCVJJzGVmcFdwJpuoap+vapWVdUq4D7gK53NT45vq6oPd+qbgQ8CK9tr/JibgD1VtRLY09YlSTPolGFQVd8CXpxoW/vt/v3APSc7RpIlwPlV9WBVFXA3cH3bvBbY1pa3deqSpBnS7z2DdwLPV9WhTm1Fku8k+Ysk72y1pcBop81oqwEsrqqjbfk5YPFkb5ZkY5LhJMNjY2N9dl2SNK7fMFjPj84KjgI/W1WXAb8NfDHJ+VM9WJs11Em2b6mq1VW1emjoVX+1TZI0TdP+0FmShcC/Bt42XquqV4BX2vK+JE8CbwKOAMs6uy9rNYDnkyypqqPtctKx6fZJkjQ9/cwMfhV4vKp+ePknyVCSBW35DfRuFD/VLgO9nOSKdp/hBuD+ttsOYENb3tCpS5JmyFQeLb0H+CvgzUlGk9zYNq3j1TeOfxnY3x41/TLw4aoav/n8EeCPgRHgSeDrrX478K4kh+gFzO3TH44kaTpOeZmoqtZPUv/NCWr30XvUdKL2w8ClE9RfAK46VT8kSWeOn0CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliCmGQZGuSY0kOdGofT3IkycPtdV1n2y1JRpI8keSaTn1Nq40k2dSpr0jyUKt/KclrBjlASdKpTWVmcBewZoL6HVW1qr12AiS5BFgHvKXt89+TLEiyAPgscC1wCbC+tQX4ZDvWzwEvATf2MyBJ0uk7ZRhU1beAF6d4vLXA9qp6paqeBkaAy9trpKqeqqr/C2wH1iYJ8CvAl9v+24DrT28IkqR+9XPP4OYk+9tlpEWtthR4ttNmtNUmq78O+PuqOn5CfUJJNiYZTjI8NjbWR9clSV3TDYPNwBuBVcBR4NOD6tDJVNWWqlpdVauHhoZm4i0laV5YOJ2dqur58eUknwMeaKtHgIs7TZe1GpPUXwAuSLKwzQ667SVJM2RaM4MkSzqr7wXGnzTaAaxLcl6SFcBK4NvAXmBle3LoNfRuMu+oqgK+Cbyv7b8BuH86fZIkTd8pZwZJ7gGuBC5KMgrcClyZZBVQwGHgQwBVdTDJvcCjwHHgpqr6QTvOzcAuYAGwtaoOtrf4D8D2JP8Z+A7w+UENTpI0NacMg6paP0F50v9hV9VtwG0T1HcCOyeoP0XvaSNJ0izxE8iSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkphCGCTZmuRYkgOd2n9N8niS/Um+muSCVl+e5B+TPNxef9jZ521JHkkykuTOJGn1C5PsTnKo/Vx0BsYpSTqJqcwM7gLWnFDbDVxaVT8P/A1wS2fbk1W1qr0+3KlvBj4IrGyv8WNuAvZU1UpgT1uXJM2gU4ZBVX0LePGE2p9X1fG2+iCw7GTHSLIEOL+qHqyqAu4Grm+b1wLb2vK2Tl2SNEMGcc/gt4Cvd9ZXJPlOkr9I8s5WWwqMdtqMthrA4qo62pafAxZP9kZJNiYZTjI8NjY2gK5LkqDPMEjye8Bx4AutdBT42aq6DPht4ItJzp/q8dqsoU6yfUtVra6q1UNDQ330XJLUtXC6Oyb5TeDXgKva/8SpqleAV9ryviRPAm8CjvCjl5KWtRrA80mWVNXRdjnp2HT7JEmanmnNDJKsAX4XeE9Vfa9TH0qyoC2/gd6N4qfaZaCXk1zRniK6Abi/7bYD2NCWN3TqkqQZcsqZQZJ7gCuBi5KMArfSe3roPGB3e0L0wfbk0C8Dn0jyfeCfgA9X1fjN54/QezLpJ+jdYxi/z3A7cG+SG4FngPcPZGSSpCk7ZRhU1foJyp+fpO19wH2TbBsGLp2g/gJw1an6IUk6c/wEsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJElMMgyRbkxxLcqBTuzDJ7iSH2s9FrZ4kdyYZSbI/yVs7+2xo7Q8l2dCpvy3JI22fO5NkkIOUJJ3cVGcGdwFrTqhtAvZU1UpgT1sHuBZY2V4bgc3QCw/gVuDtwOXAreMB0tp8sLPfie8lSTqDphQGVfUt4MUTymuBbW15G3B9p3539TwIXJBkCXANsLuqXqyql4DdwJq27fyqerCqCri7cyxJ0gzo557B4qo62pafAxa35aXAs512o612svroBPVXSbIxyXCS4bGxsT66LknqGsgN5PYbfQ3iWKd4ny1VtbqqVg8NDZ3pt5OkeaOfMHi+XeKh/TzW6keAizvtlrXayerLJqhLkmZIP2GwAxh/ImgDcH+nfkN7qugK4LvtctIu4Ooki9qN46uBXW3by0muaE8R3dA5liRpBiycSqMk9wBXAhclGaX3VNDtwL1JbgSeAd7fmu8ErgNGgO8BHwCoqheT/D6wt7X7RFWN35T+CL0nln4C+Hp7SZJmyJTCoKrWT7LpqgnaFnDTJMfZCmydoD4MXDqVvkiSBs9PIEuSDANJkmEgSWKK9wzmi+WbvjZh/fDt757hnkjSzHJmIEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJLoIwySvDnJw53Xy0k+luTjSY506td19rklyUiSJ5Jc06mvabWRJJv6HZQk6fRM+4/bVNUTwCqAJAuAI8BXgQ8Ad1TVp7rtk1wCrAPeArwe+EaSN7XNnwXeBYwCe5PsqKpHp9s3SdLpGdRfOrsKeLKqnkkyWZu1wPaqegV4OskIcHnbNlJVTwEk2d7aGgaSNEMGdc9gHXBPZ/3mJPuTbE2yqNWWAs922oy22mT1V0myMclwkuGxsbEBdV2S1HcYJHkN8B7gT1ppM/BGepeQjgKf7vc9xlXVlqpaXVWrh4aGBnVYSZr3BnGZ6Frgr6vqeYDxnwBJPgc80FaPABd39lvWapykLkmaAYO4TLSeziWiJEs6294LHGjLO4B1Sc5LsgJYCXwb2AusTLKizTLWtbaSpBnS18wgyWvpPQX0oU75vyRZBRRweHxbVR1Mci+9G8PHgZuq6gftODcDu4AFwNaqOthPvyRJp6evMKiqfwBed0LtN07S/jbgtgnqO4Gd/fRFkjR9fgJZkmQYSJIMA0kSg/sE8py2fNPXJqwfvv3dM9wTSToznBlIkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRIDCIMkh5M8kuThJMOtdmGS3UkOtZ+LWj1J7kwykmR/krd2jrOhtT+UZEO//ZIkTd2gZgb/qqpWVdXqtr4J2FNVK4E9bR3gWmBle20ENkMvPIBbgbcDlwO3jgeIJOnMO1OXidYC29ryNuD6Tv3u6nkQuCDJEuAaYHdVvVhVLwG7gTVnqG+SpBMMIgwK+PMk+5JsbLXFVXW0LT8HLG7LS4FnO/uOttpk9R+RZGOS4STDY2NjA+i6JAlg4QCO8UtVdSTJzwC7kzze3VhVlaQG8D5U1RZgC8Dq1asHckxJ0gBmBlV1pP08BnyV3jX/59vlH9rPY635EeDizu7LWm2yuiRpBvQVBklem+SnxpeBq4EDwA5g/ImgDcD9bXkHcEN7qugK4LvtctIu4Ooki9qN46tbTZI0A/q9TLQY+GqS8WN9sar+LMle4N4kNwLPAO9v7XcC1wEjwPeADwBU1YtJfh/Y29p9oqpe7LNvkqQp6isMquop4BcmqL8AXDVBvYCbJjnWVmBrP/2RJE2Pn0CWJBkGkiTDQJKEYSBJwjCQJDGYTyDPW8s3fW3C+uHb3z3DPZGk/jgzkCQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk0UcYJLk4yTeTPJrkYJKPtvrHkxxJ8nB7XdfZ55YkI0meSHJNp76m1UaSbOpvSJKk09XPV1gfB36nqv46yU8B+5LsbtvuqKpPdRsnuQRYB7wFeD3wjSRvaps/C7wLGAX2JtlRVY/20TdJ0mmYdhhU1VHgaFv+30keA5aeZJe1wPaqegV4OskIcHnbNlJVTwEk2d7anrNh4N85kHSuGcg9gyTLgcuAh1rp5iT7k2xNsqjVlgLPdnYbbbXJ6hO9z8Ykw0mGx8bGBtF1SRIDCIMkPwncB3ysql4GNgNvBFbRmzl8ut/3GFdVW6pqdVWtHhoaGtRhJWne6+vPXib5MXpB8IWq+gpAVT3f2f454IG2egS4uLP7slbjJHVJ0gyYdhgkCfB54LGq+kynvqTdTwB4L3CgLe8AvpjkM/RuIK8Evg0EWJlkBb0QWAf8m+n262w22b0E8H6CpNnVz8zgF4HfAB5J8nCr/UdgfZJVQAGHgQ8BVNXBJPfSuzF8HLipqn4AkORmYBewANhaVQf76Jck6TT18zTRX9L7rf5EO0+yz23AbRPUd55sP0nSmeUnkCVJhoEkyTCQJGEYSJLo83MGGhy/wkLSbHJmIEkyDCRJhoEkCcNAkoRhIEnCMJAk4aOlZz0fOZU0E5wZSJIMA0mSl4nOWV4+kjRIzgwkSYaBJMnLRHOOl48kTYczA0mSM4P5YrIZw2ScSUjzy1kzM0iyJskTSUaSbJrt/kjSfHJWzAySLAA+C7wLGAX2JtlRVY/Obs/mL2cS0vxyVoQBcDkwUlVPASTZDqwFDINzxOmGx2w63eDyprzmg7MlDJYCz3bWR4G3n9goyUZgY1v9P0memMZ7XQT83TT2O5c55o58cjBvMKjjDNB8PM8wP8fdz5j/xUTFsyUMpqSqtgBb+jlGkuGqWj2gLp0THPP8MB/HDPNz3GdizGfLDeQjwMWd9WWtJkmaAWdLGOwFViZZkeQ1wDpgxyz3SZLmjbPiMlFVHU9yM7ALWABsraqDZ+jt+rrMdI5yzPPDfBwzzM9xD3zMqapBH1OSdI45Wy4TSZJmkWEgSZpfYTBXv/IiycVJvpnk0SQHk3y01S9MsjvJofZzUasnyZ3t32F/krfO7gimJ8mCJN9J8kBbX5HkoTauL7WHEUhyXlsfaduXz2rH+5DkgiRfTvJ4kseSvGMenOd/3/67PpDkniQ/PtfOdZKtSY4lOdCpnfZ5TbKhtT+UZMPp9GHehEHnKy+uBS4B1ie5ZHZ7NTDHgd+pqkuAK4Cb2tg2AXuqaiWwp61D799gZXttBDbPfJcH4qPAY531TwJ3VNXPAS8BN7b6jcBLrX5Ha3eu+gPgz6rqXwK/QG/8c/Y8J1kK/DtgdVVdSu8Bk3XMvXN9F7DmhNppndckFwK30vvA7uXAreMBMiVVNS9ewDuAXZ31W4BbZrtfZ2is99P7nqcngCWttgR4oi3/EbC+0/6H7c6VF73PouwBfgV4AAi9T2QuPPF803tK7R1teWFrl9kewzTG/NPA0yf2fY6f5/FvJ7iwnbsHgGvm4rkGlgMHpntegfXAH3XqP9LuVK95MzNg4q+8WDpLfTlj2rT4MuAhYHFVHW2bngMWt+W58G/x34DfBf6prb8O+PuqOt7Wu2P64Xjb9u+29ueaFcAY8D/a5bE/TvJa5vB5rqojwKeAvwWO0jt3+5j75xpO/7z2db7nUxjMeUl+ErgP+FhVvdzdVr1fFebEc8RJfg04VlX7ZrsvM2wh8FZgc1VdBvwD///SATC3zjNAu8yxll4Qvh54La++nDLnzcR5nU9hMKe/8iLJj9ELgi9U1Vda+fkkS9r2JcCxVj/X/y1+EXhPksPAdnqXiv4AuCDJ+Acpu2P64Xjb9p8GXpjJDg/IKDBaVQ+19S/TC4e5ep4BfhV4uqrGqur7wFfonf+5fq7h9M9rX+d7PoXBnP3KiyQBPg88VlWf6WzaAYw/UbCB3r2E8foN7amEK4DvdqajZ72quqWqllXVcnrn8X9V1b8Fvgm8rzU7cbzj/w7va+3Pud+eq+o54Nkkb26lq+h9zfucPM/N3wJXJPnn7b/z8THP6XPdnO553QVcnWRRm1Fd3WpTM9s3TWb4Bs11wN8ATwK/N9v9GeC4foneFHI/8HB7XUfvWuke4BDwDeDC1j70nqx6EniE3pMasz6OaY79SuCBtvwG4NvACPAnwHmt/uNtfaRtf8Ns97uP8a4Chtu5/lNg0Vw/z8B/Ah4HDgD/Ezhvrp1r4B5690S+T28GeON0zivwW23sI8AHTqcPfh2FJGleXSaSJE3CMJAkGQaSJMNAkoRhIEnCMJAkYRhIkoD/B2bSj+h7gwQKAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(statements_hist, bins=50)\n",
    "print(np.mean(statements_hist))\n",
    "print(max_code_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.168803995753732\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(nodes_hist, bins=20)\n",
    "print(np.mean(nodes_hist))\n",
    "print(max_statement_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_embedding(shape: tuple, vocab: list, input_size: int, output_size: int, shift: int = 0, pre_trained = None):\n",
    "    embedding = tf.keras.layers.Embedding(input_size, output_size, name='Tree_Embedding', mask_zero=True)\n",
    "    embedding.build(shape)\n",
    "    embedding.trainable = pre_trained is None\n",
    "    if pre_trained is not None:\n",
    "        weights = np.zeros((input_size, output_size))\n",
    "        for i, token in enumerate(vocab):\n",
    "            try:\n",
    "                weights[i + shift] = pre_trained[i]\n",
    "            except:\n",
    "                pass\n",
    "        embedding.set_weights([weights])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_embedding_encoder(input_shape: tuple, output_size: int):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    outputs = tf.keras.layers.Dense(output_size)(inputs)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='Embedding_Encoder')\n",
    "\n",
    "\n",
    "def get_statement_encoder(input_shape: tuple, filters: int):\n",
    "    inputs = tf.keras.layers.Input(input_shape)\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=filters, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Lambda(lambda tensor: tf.math.reduce_max(tensor, axis=2))(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name='Statement_Encoder')\n",
    "\n",
    "\n",
    "def get_tree_encoder(input_shape: tuple, predict: bool = False, backbone: str = 'rnn'):\n",
    "    inputs = tf.keras.layers.Input(input_shape, name='Inputs')\n",
    "    dropout = tf.keras.layers.Dropout(0.1, name='Embedding_Dropout')(inputs)\n",
    "    if backbone == 'rnn':\n",
    "        x = tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.RNN(tf.keras.layers.GRUCell(units=128)),\n",
    "            name='Double_Bidirectional_GRU')(dropout)\n",
    "    else:\n",
    "        n_layers = 2\n",
    "        kernels = [3, 5, 7]\n",
    "        layers = []\n",
    "        for k in kernels:\n",
    "            x = dropout\n",
    "            n = x.shape[-1]\n",
    "            for i in range(n_layers):\n",
    "                x = tf.keras.layers.Conv1D(n, k, activation=tf.keras.activations.swish, padding='same', name=f'Conv1D_{k}_{n}')(x)\n",
    "                x = tf.keras.layers.BatchNormalization(name=f'Batch_Norm_{k}_{n}')(x)\n",
    "                n *= 2\n",
    "            x = tf.keras.layers.GlobalMaxPooling1D(name=f'Max_Pool_{k}')(x)\n",
    "            layers.append(x)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1, name='Pool_Concatenate')(layers)\n",
    "        x = tf.keras.layers.Dropout(0.2, name='Concatenate_Dropout')(x)\n",
    "    if predict:\n",
    "        x = tf.keras.layers.Dense(units=len(labels), activation='sigmoid', name='Prediction')(x)\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=x, name='Tree_Encoder')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class StatementDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, vectorized, indices, he_tags, batch_size):\n",
    "        self.vectorized = vectorized\n",
    "        self.indices = indices\n",
    "        self.he_tags = he_tags\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.vectorized) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "        vectorized_batch = vectorized[start:end]\n",
    "        indices_batch = indices[start:end]\n",
    "        he_tags_batch = he_tags[start:end]\n",
    "\n",
    "        max_len = max(len(j) for i in vectorized_batch for j in i)\n",
    "        code_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [tf.keras.preprocessing.sequence.pad_sequences(i, padding='post', maxlen=max_len) for i in vectorized_batch],\n",
    "            padding='post'\n",
    "        )\n",
    "        code_batch = code_batch[:,1:129]\n",
    "        indices_batch = [[i[j] for j in range(1, min(129, len(i)))] for i in indices_batch]\n",
    "        return (code_batch, indices_batch), he_tags_batch\n",
    "\n",
    "    def shuffle(self):\n",
    "        indices = np.arange(len(self.vectorized))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        self.vectorized = [self.vectorized[i] for i in indices]\n",
    "        self.indices = [self.indices[i] for i in indices]\n",
    "        self.he_tags = [self.he_tags[i] for i in indices]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.shuffle()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "shift = 1\n",
    "emb_input_size = len(vocab) + shift\n",
    "emb_output_size = 192\n",
    "emb_encoder_output_size = 128\n",
    "backbone = 'cnn'\n",
    "path = f'java_statements_model_revisited/{backbone}'\n",
    "\n",
    "def calc_logits(code_batch, indices_batch, embedding, embedding_encoder_model, tree_encoder_model):\n",
    "    # b: code_batch length\n",
    "    # n: code_batch.shape[1] -- number of sequences in code\n",
    "    # m: code_batch.shape[2] -- number of statements in sequence\n",
    "    # e: embedding vector size\n",
    "    # code_batch: (b, n, m,)\n",
    "    # encoded: (b, n, m, e,)\n",
    "    encoded = embedding_encoder_model(embedding(code_batch), training=True)\n",
    "    encoded_as_list = [[[encoded[i, j, k] for k in range(code_batch.shape[2])] for j in range(code_batch.shape[1])] for i in range(len(code_batch))]\n",
    "    # statement encoding over known embeddings\n",
    "    for i in range(len(indices_batch)):\n",
    "        for j in range(len(indices_batch[i])):\n",
    "            for k in range(len(indices_batch[i][j]) - 1, -1, -1):\n",
    "                children = indices_batch[i][j][k]\n",
    "                if not children:\n",
    "                    continue\n",
    "                encoded_as_list[i][j][k] = tf.reduce_sum([\n",
    "                    encoded_as_list[i][j][k],\n",
    "                    *(encoded_as_list[i][j][l] for l in children)\n",
    "                ], axis=0)\n",
    "    pooled = tf.math.reduce_max(encoded_as_list, axis=2)\n",
    "    logits = tree_encoder_model(pooled, training=True)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def calc_loss(y_true, y_pred, metrics=None):\n",
    "    metrics = metrics or []\n",
    "    loss = tf.math.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "    for metric in metrics:\n",
    "        metric.update_state(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def tree_encoding_step(optimizer, code_batch, indices_batch, he_tags_batch, metrics):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = calc_logits(code_batch, indices_batch,\n",
    "                             embedding, embedding_encoder_model, tree_encoder_model)\n",
    "        loss = calc_loss(he_tags_batch, logits, metrics)\n",
    "        trainable_weights = tree_encoder_model.trainable_weights + embedding_encoder_model.trainable_weights\n",
    "    grads = tape.gradient(loss, trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def models_parallel_train(epochs: int, batch_size: int):\n",
    "    # split data\n",
    "    vectorized_train, vectorized_test, \\\n",
    "    indices_train, indices_test, \\\n",
    "    he_tags_train, he_tags_test = \\\n",
    "        sklearn.model_selection.train_test_split(vectorized,\n",
    "                                                 indices,\n",
    "                                                 he_tags)\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = StatementDataset(vectorized_train, indices_train, he_tags_train, batch_size=batch_size)\n",
    "    test_dataset = StatementDataset(vectorized_test, indices_test, he_tags_test, batch_size=batch_size)\n",
    "\n",
    "    # create handlers for logging\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    train_loss_log = open(os.path.join(path, 'train_loss.txt'), 'w', encoding='utf-8')\n",
    "    train_f1_log   = open(os.path.join(path, 'train_f1.txt'),   'w', encoding='utf-8')\n",
    "    test_loss_log  = open(os.path.join(path, 'test_loss.txt'),  'w', encoding='utf-8')\n",
    "    test_f1_log    = open(os.path.join(path, 'test_f1.txt'),    'w', encoding='utf-8')\n",
    "\n",
    "    try:\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=2e-3)\n",
    "        metrics = [tfa.metrics.F1Score(len(labels), average='macro')]\n",
    "        for epoch in tqdm.trange(epochs):\n",
    "\n",
    "            #train\n",
    "            loss = 0\n",
    "            pbar = tqdm.tqdm(enumerate(train_dataset), total=len(train_dataset))\n",
    "            for step, ((code_batch, indices_batch), he_tags_batch) in pbar:\n",
    "                loss_value = tree_encoding_step(optimizer,\n",
    "                                                code_batch,\n",
    "                                                indices_batch,\n",
    "                                                he_tags_batch,\n",
    "                                                metrics)\n",
    "                loss += loss_value.numpy()\n",
    "                mean_loss = f'{loss / (step + 1):.3f}'\n",
    "                f1_score = f'{metrics[0].result().numpy():.3f}'\n",
    "                pbar.set_description(f'[TRAIN] | Loss: {mean_loss}; F1: {f1_score}')\n",
    "                if step % 100 == 0:\n",
    "                    train_loss_log.write(f'{mean_loss} ')\n",
    "                    train_f1_log.write(f'{f1_score} ')\n",
    "            train_loss_log.write('\\n')\n",
    "            train_f1_log.write('\\n')\n",
    "\n",
    "            # save\n",
    "            embedding_encoder_model.save(os.path.join(path, f'embedding_encoder_weights_{total_epochs + epoch}'))\n",
    "            tree_encoder_model.save(os.path.join(path, f'statement_tree_encoder_weights_{total_epochs + epoch}'))\n",
    "\n",
    "            # reset metrics\n",
    "            for metric in metrics:\n",
    "                metric.reset_state()\n",
    "\n",
    "            # test\n",
    "            loss = 0\n",
    "            pbar = tqdm.tqdm(enumerate(test_dataset), total=len(test_dataset))\n",
    "            for step, ((code_batch, indices_batch), he_tags_batch) in pbar:\n",
    "                logits = calc_logits(code_batch, indices_batch,\n",
    "                                     embedding, embedding_encoder_model, tree_encoder_model)\n",
    "                loss_value = calc_loss(he_tags_batch, logits, metrics)\n",
    "                loss += loss_value.numpy()\n",
    "                mean_loss = f'{loss / (step + 1):.3f}'\n",
    "                f1_score = f'{metrics[0].result().numpy():.3f}'\n",
    "                pbar.set_description(f'[TEST] | Loss: {mean_loss}; F1: {f1_score}')\n",
    "\n",
    "            test_loss_log.write(f'{loss / len(test_dataset)}\\n')\n",
    "            test_f1_log.write(f'{metrics[0].result().numpy():.3f}\\n')\n",
    "\n",
    "            # reset metrics\n",
    "            for metric in metrics:\n",
    "                metric.reset_state()\n",
    "\n",
    "            train_loss_log.flush()\n",
    "            train_f1_log.flush()\n",
    "            test_loss_log.flush()\n",
    "            test_f1_log.flush()\n",
    "            train_dataset.on_epoch_end()\n",
    "    finally:\n",
    "        train_loss_log.close()\n",
    "        train_f1_log.close()\n",
    "        test_loss_log.close()\n",
    "        test_f1_log.close()\n",
    "\n",
    "\n",
    "embedding = get_embedding(shape=(None, None, None),\n",
    "                          input_size=emb_input_size,\n",
    "                          output_size=emb_output_size,\n",
    "                          pre_trained=ft_embeddings,\n",
    "                          shift=1,\n",
    "                          vocab=vocab)\n",
    "embedding_encoder_model = get_embedding_encoder(input_shape=(None, None, emb_output_size),\n",
    "                                                output_size=emb_encoder_output_size)\n",
    "tree_encoder_model = get_tree_encoder(input_shape=(None, emb_encoder_output_size),\n",
    "                                      predict=True, backbone=backbone)\n",
    "\n",
    "total_epochs = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_3_128_layer_call_and_return_conditional_losses_1294348935) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_5_128_layer_call_and_return_conditional_losses_1294350725) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_7_128_layer_call_and_return_conditional_losses_1294350755) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_1294347835) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_5_256_layer_call_and_return_conditional_losses_1294349016) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_7_128_layer_call_and_return_conditional_losses_1294348881) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Tree_Encoder_layer_call_and_return_conditional_losses_1294350638) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_3_256_layer_call_and_return_conditional_losses_1294349043) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_5_128_layer_call_and_return_conditional_losses_1294348908) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_3_256_layer_call_and_return_conditional_losses_1294351025) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_7_256_layer_call_and_return_conditional_losses_1294348989) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_3_128_layer_call_and_return_conditional_losses_1294350695) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_7_256_layer_call_and_return_conditional_losses_1294351085) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Conv1D_5_256_layer_call_and_return_conditional_losses_1294351055) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_Tree_Encoder_layer_call_and_return_conditional_losses_1294350321) with ops with custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# total_epochs = 4\n",
    "# embedding_encoder_model = tf.keras.models.load_model(f'{path}/embedding_encoder_weights_{total_epochs - 1}')\n",
    "# tree_encoder_model = tf.keras.models.load_model(f'{path}/statement_tree_encoder_weights_{total_epochs - 1}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6aee845f11e40ac8f307045e04b93c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9101 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea95bf02cf994c08b39057a9d2929327"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: java_statements_model_revisited/cnn\\embedding_encoder_weights_0\\assets\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Inputs with unsupported characters which will be renamed to inputs in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: java_statements_model_revisited/cnn\\statement_tree_encoder_weights_0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: java_statements_model_revisited/cnn\\statement_tree_encoder_weights_0\\assets\n",
      "d:\\git\\yepcode\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:254: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  warnings.warn('Metric %s implements a `reset_states()` method; rename it '\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3034 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6453530c04ed4be0a225ceb9703d888f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9101 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd08d0e26384481cb5cd595a12ed19e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models_parallel_train(epochs=5, batch_size=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def get_combined_model(input_shape: tuple, embedding, encoder, statement_encoder, tree_encoder):\n",
    "#     inputs = tf.keras.layers.Input(input_shape, name='Statement_Trees')\n",
    "#     x = embedding(inputs)\n",
    "#     x = encoder(x)\n",
    "#     x = statement_encoder(x)\n",
    "#     x = tree_encoder(x)\n",
    "#     x = tf.keras.layers.Dense(units=256, activation=tf.keras.activations.swish, name='Dense')(x)\n",
    "#     x = tf.keras.layers.Dense(units=len(labels), activation='sigmoid', name='Prediction')(x)\n",
    "#     model = tf.keras.models.Model(inputs=inputs, outputs=x, name='PSLNN_Combined')\n",
    "#     model.summary()\n",
    "#     model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "#     return model\n",
    "\n",
    "# def statement_encoding(epochs: int, batch_size: int):\n",
    "#     loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "#     optimizer = tf.keras.optimizers.Adam()\n",
    "#\n",
    "#     for epoch in tqdm.trange(epochs):\n",
    "#         vectorized_batches = batch(vectorized, batch_size)\n",
    "#         indices_batches = batch(indices, batch_size)\n",
    "#         he_tags_batches = batch(he_tags, batch_size)\n",
    "#         pbar = tqdm.tqdm(enumerate(zip(vectorized_batches, indices_batches, he_tags_batches)), total=len(vectorized) // batch_size)\n",
    "#         for step, (vectorized_batch, indices_batch, he_tags_batch) in pbar:\n",
    "#             max_len = max(len(j) for i in vectorized_batch for j in i)\n",
    "#             code_batch = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#                 [tf.keras.preprocessing.sequence.pad_sequences(i, padding='post', maxlen=max_len) for i in vectorized_batch],\n",
    "#                 padding='post'\n",
    "#             )\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 # m: max_len\n",
    "#                 # code_batch: b x n x m\n",
    "#                 # encoded: b x n x m x e\n",
    "#                 encoded = encoder_model(embedding(code_batch), training=True)\n",
    "#                 pooled = statement_encoder_model(encoded, training=True)\n",
    "#                 #logits = model(pooled, training=True)\n",
    "#                 #loss_value = loss_fn(he_tags_batch, logits)\n",
    "#\n",
    "#             #grads = tape.gradient(loss_value, model.trainable_weights + encoder_model.trainable_weights)\n",
    "#             #optimizer.apply_gradients(zip(grads, model.trainable_weights + encoder_model.trainable_weights))\n",
    "#             #pbar.set_description(str(loss_value.numpy()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# class FlatteningDataset(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, vectorized, indices, he_tags, batch_size):\n",
    "#         self.vectorized = vectorized\n",
    "#         self.indices = indices\n",
    "#         self.he_tags = he_tags\n",
    "#         self.batch_size = batch_size\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return (len(self.vectorized) + self.batch_size - 1) // self.batch_size\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         start = idx * self.batch_size\n",
    "#         end = (idx + 1) * self.batch_size\n",
    "#         vectorized_batch = vectorized[start:end]\n",
    "#         indices_batch = indices[start:end]\n",
    "#         he_tags_batch = he_tags[start:end]\n",
    "#\n",
    "#         vectorized_batch = [[st for sequence in code for st in sequence] for code in vectorized_batch]\n",
    "#         code_batch = tf.keras.preprocessing.sequence.pad_sequences(vectorized_batch, maxlen=1024, padding='post')\n",
    "#         return code_batch, np.array(he_tags_batch)\n",
    "#\n",
    "#     def on_epoch_end(self):\n",
    "#         pass\n",
    "\n",
    "# shift = 1\n",
    "# emb_input_size = len(vocab) + shift\n",
    "# emb_output_size = 192\n",
    "#\n",
    "# flat_model = get_flat_model(input_shape=(None,),\n",
    "#                             embedding=get_embedding(shape=(None, None),\n",
    "#                                                     vocab=vocab,\n",
    "#                                                     input_size=emb_input_size,\n",
    "#                                                     output_size=emb_output_size,\n",
    "#                                                     shift=shift,\n",
    "#                                                     pre_trained=ft_embeddings))\n",
    "#\n",
    "# flat_dataset = FlatteningDataset(vectorized, indices, he_tags, batch_size=8)\n",
    "#\n",
    "# emb_encoder_output_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}